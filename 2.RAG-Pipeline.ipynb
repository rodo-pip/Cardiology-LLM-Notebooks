{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff89aee-a820-447e-bb96-3bd306df522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Δημιουργία client για το Bedrock\n",
    "bedrock = boto3.client(\"bedrock\", region_name=\"us-west-2\")\n",
    "\n",
    "# Λίστα με όλα τα  διαθέσιμα foundation models\n",
    "models = bedrock.list_foundation_models()\n",
    "\n",
    "# Εκτύπωση μοντέλων llama\n",
    "for model in models[\"modelSummaries\"]:\n",
    "    if \"llama\" in model[\"modelId\"].lower():\n",
    "        print(model[\"modelId\"], \"-\", model[\"providerName\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfb613-8602-4af1-b0b1-799be9d054c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d8631-2d42-453a-a217-b36b209a42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fe901-7b02-4e86-8eda-2763fb017f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cb7ef-5e0c-426e-ac2d-d00a957cbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4baf21-181a-4c7a-a0e9-1771cbad32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044c725-6573-4ffd-b900-6d478e9f60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b75f6-fd40-45d7-9836-8cf6e0b84756",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9fcd7-fe5b-4da9-871e-b702df06faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08003d3f-dad2-4dd1-914c-43c4971d65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf==1.23.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7010799-ac19-4c0b-9bed-bb80919c5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "print(fitz.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61160c-3177-4585-b89f-24d263940022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import torch\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Ορισμός Hugging Face token\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"****\"\n",
    "\n",
    "# Ρυθμίσεις AWS S3\n",
    "aws_access_key = \"\"\n",
    "aws_secret_key = \"\"\n",
    "region = \"us-west-2\"\n",
    "bucket = \"rag-deployment\"\n",
    "prefix = \"rag_docs/\"\n",
    "\n",
    "# Σύνδεση με AWS S3\n",
    "s3 = boto3.client(\"s3\",\n",
    "                  aws_access_key_id=aws_access_key,\n",
    "                  aws_secret_access_key=aws_secret_key,\n",
    "                  region_name=region)\n",
    "\n",
    "# Ανάκτηση της λίστα των PDF αρχείων \n",
    "pdf_keys = []\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "while True:\n",
    "    contents = response.get(\"Contents\", [])\n",
    "    pdf_keys.extend([obj[\"Key\"] for obj in contents if obj[\"Key\"].endswith(\".pdf\")])\n",
    "    if response.get(\"IsTruncated\"):\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix,\n",
    "            ContinuationToken=response[\"NextContinuationToken\"]\n",
    "        )\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Βρέθηκαν {len(pdf_keys)} PDF.\")\n",
    "\n",
    "# Κατέβασμα και ανάγνωση των PDF\n",
    "docs = []\n",
    "start = time.time()\n",
    "for i, key in enumerate(pdf_keys[:2], 1):\n",
    "    print(f\"[{i}/2] Λήψη: {key}\")\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        pdf_bytes = obj[\"Body\"].read()\n",
    "        pdf = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "        text = \"\\n\".join([page.get_text() for page in pdf])\n",
    "        if text.strip():\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": key}))\n",
    "        else:\n",
    "            print(f\" Άδειο ή μη αναγνώσιμο: {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Σφάλμα στο {key}: {e}\")\n",
    "print(f\"Ολοκληρώθηκε η ανάκτηση PDF σε {round(time.time()-start, 2)} sec.\")\n",
    "\n",
    "# Διάσπαση κειμένου σε Chunks\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=int(0.2 * 500)) \n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"Συνολικός αριθμός chunks: {len(split_docs)}\")\n",
    "\n",
    "#  Προσθήκη prefix \"passage:\" για καλύτερη ανάκτηση\n",
    "for doc in split_docs:\n",
    "    doc.page_content = \"passage: \" + doc.page_content.strip()\n",
    "\n",
    "# Αρχικοποίηση διανυσματικού μοντέλου \n",
    "start = time.time()\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embedding=embeddings)\n",
    "print(f\" Embeddings ολοκληρώθηκαν σε {round(time.time()-start, 2)} sec.\")\n",
    "\n",
    "# Αποθήκευση vector store\n",
    "db.save_local(\"e5largev2_rag_db\")\n",
    "print(\"Ο vector store αποθηκεύτηκε τοπικά στο: e5largev2_rag_db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acab587-e4f9-4a01-89ac-04d18c15eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Συνέχεια επεξεργασίας των PDF, από το τριτο \n",
    "\n",
    "# Ορισμός διανυσματικού μοντέλου\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=int(0.2 * 500))\n",
    "\n",
    "for i, key in enumerate(pdf_keys[:], start=0):\n",
    "    print(f\"\\n [{i}/{len(pdf_keys)}] Επεξεργασία: {key}\")\n",
    "\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        pdf_bytes = obj[\"Body\"].read()\n",
    "        pdf = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "        text = \"\\n\".join([page.get_text() for page in pdf])\n",
    "        if not text.strip():\n",
    "            print(\"Άδειο ή μη αναγνώσιμο αρχείο, παραλείπεται.\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Σφάλμα ανάγνωσης: {e}\")\n",
    "        continue\n",
    "\n",
    "    # === Διάσπαση αρχείων σε chunks\n",
    "    doc = Document(page_content=text, metadata={\"source\": key})\n",
    "    split_docs = splitter.split_documents([doc])\n",
    "    for d in split_docs:\n",
    "        d.page_content = \"passage: \" + d.page_content.strip()\n",
    "    print(f\"Chunks που θα προστεθούν: {len(split_docs)}\")\n",
    "\n",
    "    # === Φόρτωση FAISS, προσθήκη ν΄εων chunks και αποθήκευση\n",
    "    try:\n",
    "        db = FAISS.load_local(\"e5largev2_rag_db\", embeddings, allow_dangerous_deserialization=True)\n",
    "        db.add_documents(split_docs)\n",
    "        db.save_local(\"e5largev2_rag_db\")\n",
    "        print(f\"Προστέθηκαν στο FAISS: {len(split_docs)} chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Σφάλμα κατά την ενημέρωση FAISS: {e}\")\n",
    "        continue\n",
    "print(\"\\n Η λούπα ολοκληρώθηκε.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac9616-c83c-4b1e-822f-502a6fefcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137fcb8-808b-4eeb-97d2-9b679f0522d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωση FAISS retriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch \n",
    "\n",
    "# Ίδιο embeddings με αυτά που έκανες store\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Φόρτωση FAISS index και μετατροπή σε retriever\n",
    "retriever = FAISS.load_local(\"e5largev2_rag_db\", embeddings, allow_dangerous_deserialization=True).as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 15}  \n",
    ")\n",
    "print(\" Ο retriever φορτώθηκε επιτυχώς.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee150706-398b-4808-bb08-f5bb7324471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vectors = retriever.vectorstore.index.ntotal\n",
    "print(f\"Βρέθηκαν {num_vectors} αποθηκευμένα embeddings στο FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989ffb0-30a6-4560-9faa-6c02ac4473f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Παράδειγμα ερώτησης προς τον retriever και εμφάνιση του πιο σχετικού αποσπάσματος\n",
    "query = \"What are the causes of elevated troponin?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(\"\\nΤα πιο σχετικά αποσπάσματα:\\n\", docs[0].page_content[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756f733-32b0-4fea-aa55-ca3f0870c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c032f61-9189-44e2-83dc-624a9138e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ανέβασμα evaluation dataset\n",
    "validation_path = r\"sagemaker_ft500vers2 (1) (1).jsonl\"\n",
    "with open(validation_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "validation_data = []\n",
    "for i, line in enumerate(lines):\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        if \"model_input\" in data:\n",
    "            validation_data.append(data)\n",
    "        else:\n",
    "            print(f\"[Missing 'model_input' in line {i}]: {data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error parsing line {i}]: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "validation_data = validation_data[:]\n",
    "\n",
    "print(f\"✅ Loaded {len(validation_data)} valid items.\")\n",
    "\n",
    "# === 2. Bedrock client ===\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "#model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "#model_id=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/y9xa363z3o7r\"\n",
    "\n",
    "model_id=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/1tfsf1vs44wd\"\n",
    "\n",
    "\n",
    "# Prompt templates\n",
    "prompt_template = \"\"\"You are a medical question answering agent.\n",
    "\n",
    "You will receive:\n",
    "\n",
    "A multiple-choice clinical question\n",
    "\n",
    "A set of search results from medical guidelines or trusted sources\n",
    "\n",
    "Your task is to:\n",
    "- Choose the most appropriate answer (A, B, C, D, or E)\n",
    "- Justify your choice with a short explanation (max 2 sentences)\n",
    "\n",
    "If the search results are clearly related, use them to support your answer.\n",
    "If not, apply clinical reasoning consistent with current medical guidelines.\n",
    "Never skip an answer – always choose the best option.\n",
    "\n",
    "Here are the search results:\n",
    "$search_results$\n",
    "\n",
    "Here is the question:\n",
    "$query$\n",
    "\n",
    "Respond in this format:\n",
    "Answer Letter: <A/B/C/D/E> Justification: <one- or two-sentence explanation, no more than 30 words>\n",
    "\"\"\"\n",
    "\n",
    "orchestration_prompt_template = \"\"\"You are a clinical information retrieval agent.\n",
    "\n",
    "Your task is to generate an effective search query based on a multiple-choice clinical question. Focus on extracting the key medical terms, drug names, diagnostic procedures, lab findings, and clinical reasoning patterns from the question and, if useful, the answer choices.\n",
    "\n",
    "Your goal is to retrieve the most relevant passages from trusted medical guidelines or textbooks that could help identify the correct answer.\n",
    "\n",
    "Instructions:\n",
    "- Include terms that reflect the diagnostic logic or pathophysiology in the question\n",
    "- Avoid copying the full question\n",
    "- Infer important clinical concepts or conditions even if not stated directly\n",
    "- Consider keywords from the answer options if they help sharpen the search\n",
    "\n",
    "Example:\n",
    "A 40-year-old man with exertional chest pain is found to have ST elevations in V2-V4. What is the most likely cause?\n",
    "<generated_query> STEMI anterior MI ST elevation V2-V4 </generated_query>\n",
    "\n",
    "Note: Ignore any prior conversation history. Each question should be treated independently.\n",
    "\n",
    "Here is the question:\n",
    "$query$\n",
    "\"\"\"\n",
    "\n",
    "# === 4. Εκτέλεσε αξιολόγηση ===\n",
    "output_path = \"validation_answers.jsonl\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for item in tqdm(validation_data, desc=\"Processing questions\"):\n",
    "        question = item[\"model_input\"]\n",
    "        ground_truth = item.get(\"target_output\", \"\")\n",
    "\n",
    "        #  Reformulated query μέσω orchestration prompt\n",
    "        orchestration_prompt = orchestration_prompt_template.replace(\"$query$\", question)\n",
    "\n",
    "        query_body = json.dumps({\n",
    "            \"prompt\": orchestration_prompt,\n",
    "            \"max_gen_len\": 512,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.9\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            query_response = bedrock.invoke_model(\n",
    "                body=query_body,\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            reformulated_query = json.loads(query_response[\"body\"].read())[\"generation\"]\n",
    "        except Exception as e:\n",
    "            reformulated_query = question\n",
    "\n",
    "        # Retrieval από τον retriever\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(reformulated_query)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs[:15]])\n",
    "\n",
    "                # Προβολή του πρώτου chunk \n",
    "            if docs:\n",
    "                first_chunk = docs[0].page_content\n",
    "               # print(\" Πρώτο chunk:\")\n",
    "               # print(first_chunk[:1000])  # ή όλο το chunk αν προτιμάς\n",
    "            else:\n",
    "                print(\"⚠️ Δεν βρέθηκαν chunks.\")\n",
    "        except Exception as e:\n",
    "            context = f\"[ERROR in retrieval] {str(e)}\"\n",
    "\n",
    "        # === 4.3 Prompt προς LLM ===\n",
    "        prompt = prompt_template.replace(\"$search_results$\", context).replace(\"$query$\", question)\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_gen_len\": 500,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.9\n",
    "        })\n",
    "\n",
    "        # LLM απάντηση\n",
    "        try:\n",
    "            response = bedrock.invoke_model(\n",
    "                body=body,\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            generated_answer = response_body[\"generation\"]\n",
    "        except Exception as e:\n",
    "            generated_answer = f\"[ERROR in LLM] {str(e)}\"\n",
    "\n",
    "\n",
    "        #CHECK PIPELINE\n",
    "       # print(\"\\n====================\")\n",
    "       # print(\" Reformulated query:\\n\", reformulated_query)\n",
    "       # print(\" Retrieved Context:\")\n",
    "       # print(context[:1000])  \n",
    "     \n",
    "\n",
    "        # 4.5 Αποθήκευση\n",
    "        output = {\n",
    "            \"question\": question,\n",
    "            \"reformulated_query\": reformulated_query,\n",
    "            \"context_used\": context,\n",
    "            \"model_answer\": generated_answer,\n",
    "            \"ground_truth\": ground_truth\n",
    "        }\n",
    "        outfile.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "print(f\"\\nCompleted! Answers saved in: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
