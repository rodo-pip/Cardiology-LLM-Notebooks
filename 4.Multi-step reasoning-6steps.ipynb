{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8996-7be2-406e-8c00-69667ca7f4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install boto3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce14f95-0882-4c32-8f38-acb46979d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0ecc-7b88-441d-a38c-7ce1f5178e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain[bedrock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a212-16f4-4692-9dfc-3255f1791c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28b5cd-fcc9-4972-8a3d-c2ab060bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import boto3\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class BedrockLLM(BaseLLM):\n",
    "    model_id: str = Field(...)\n",
    "    region: str = Field(...)\n",
    "\n",
    "    def __init__(self, model_arn: str, region: str = \"us-west-2\"):\n",
    "        super().__init__(model_id=model_arn, region=region)\n",
    "        object.__setattr__(self, \"client\", boto3.client(\"bedrock-runtime\", region_name=region))\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"bedrock-custom\"\n",
    "\n",
    "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    body = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"max_gen_len\": 512,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.9\n",
    "                    }\n",
    "                    response = self.client.invoke_model(\n",
    "                        modelId=self.model_id,\n",
    "                        body=json.dumps(body).encode(\"utf-8\"),\n",
    "                        contentType=\"application/json\",\n",
    "                        accept=\"application/json\"\n",
    "                    )\n",
    "                    result = json.loads(response[\"body\"].read())\n",
    "                    text = result.get(\"generation\") or result.get(\"outputs\", [{}])[0].get(\"text\", str(result))\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "                except self.client.exceptions.ModelNotReadyException:\n",
    "                    wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                    print(f\"Model not ready. Retrying in {wait:.1f} seconds...\")\n",
    "                    time.sleep(wait)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Client error during invoke: {str(e)}\")\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "# Φόρτωση Δεδομένων\n",
    "with open(\"sagemaker_ft500vers2 (1) (1).jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# Ορισμός Διανυσματικού μοντέλου \n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\n",
    "\n",
    "# Φόρτωση Ανακτητή\n",
    "retriever = FAISS.load_local(\n",
    "    \"sapbert_base_db\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever()\n",
    "\n",
    "# Επιλογή LLM\n",
    "llm = BedrockLLM(\n",
    "    model_arn = \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/y9xa363z3o7r\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/1tfsf1vs44wd\",\n",
    "    region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# === Prompt Templates ===\n",
    "prompt_step1 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "You are a clinical summarization expert. Carefully read the case and provide a concise summary of only the relevant clinical findings, omitting distractors.\n",
    "\n",
    "CASE AND QUESTION:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step2 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are a senior clinician. Using the summary and current guideline-based knowledge, identify:\n",
    "\n",
    "1. The most likely diagnosis (**bold**).\n",
    "2. 2–3 plausible differential diagnoses with one-line justifications.\n",
    "3. A logical, step-by-step reasoning for your top diagnosis.\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "DIAGNOSTIC REASONING:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step3 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are an expert clinician using the VINDICATE + P² mnemonic to evaluate other possible diagnoses.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "INITIAL DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "For each category below, state one plausible alternative diagnosis (or \"Not applicable\"):\n",
    "\n",
    "- Vascular, Infectious, Neoplastic, Degenerative, Intoxication/Iatrogenic, Congenital, Autoimmune/Allergic, Trauma, Endocrine/Metabolic, Psychiatric, Paraneoplastic.\n",
    "\n",
    "Conclude by stating if any alternative is more likely than the initial.\n",
    "\n",
    "VINDICATE + P² DIFFERENTIAL:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step4 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are designing a clinical management plan.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "Provide:\n",
    "\n",
    "1. Short-term treatment steps\n",
    "2. Long-term management goals\n",
    "3. Relevant clinical scores or red flags\n",
    "\n",
    "CLINICAL PLAN:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step5 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"diagnosis\", \"vindicate\", \"management\"],\n",
    "    template=\"\"\"\n",
    "You are answering a multiple-choice medical exam question based strictly on the clinical information and reasoning below.\n",
    "\n",
    "CASE AND QUESTION:\n",
    "{question}\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSTIC REASONING:\n",
    "{diagnosis}\n",
    "\n",
    "DIFFERENTIAL ANALYSIS (VINDICATE):\n",
    "{vindicate}\n",
    "\n",
    "CLINICAL MANAGEMENT PLAN:\n",
    "{management}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- There are exactly 5 answer choices (A–E). Select only one.\n",
    "- Start your answer with the chosen letter (e.g., \"C.\") on the first line — no explanation yet.\n",
    "- Then, write 1–3 concise sentences justifying your choice.\n",
    "- Briefly reject the other 4 options.\n",
    "- Use ONLY the above information. Do NOT invent facts or make assumptions.\n",
    "\n",
    "FORMAT STRICTLY:\n",
    "<Letter>\n",
    "<Explanation>\n",
    "<Why others are incorrect>\n",
    "\n",
    "FINAL ANSWER:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_step6 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"diagnosis\", \"management\", \"answer\"],\n",
    "    template=\"\"\"\n",
    "You are a medical board evaluator. Review the following:\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "MANAGEMENT:\n",
    "{management}\n",
    "\n",
    "FINAL ANSWER:\n",
    "{answer}\n",
    "\n",
    "EVALUATE:\n",
    "- Is the answer choice consistent with the diagnosis and plan?\n",
    "- Is the reasoning complete and appropriate?\n",
    "- Are other options addressed clearly?\n",
    "\n",
    "Respond with only one of:\n",
    "- VALID: The final answer is correct and well justified.\n",
    "- INVALID: The answer is inconsistent or the justification is inadequate.\n",
    "- INCOMPLETE: The reasoning is partially correct but lacks completeness or clarity.\n",
    "\n",
    "EVALUATION:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# αΞΙΟΛΌΓΗΣΗ\n",
    "output_path = \"validation_answers_sapbert_6_steps.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for example in tqdm(dataset):\n",
    "        question = example[\"model_input\"]\n",
    "\n",
    "        # Βήμα 1: Περίληψη του κλινικού περιστατικού\n",
    "        step1_output = LLMChain(llm=llm, prompt=prompt_step1).invoke({\n",
    "            \"question\": question\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 2: Διάγνωση (με χρήση RAG και σχετικών αποσπασμάτων)\n",
    "        context2 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output)[:15]])\n",
    "        step2_output = LLMChain(llm=llm, prompt=prompt_step2).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"context\": context2\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 3: VINDICATE – συστηματικός έλεγχος πιθανών αιτιών (με RAG)\n",
    "        context3 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output + step2_output)[:15]])\n",
    "        step3_output = LLMChain(llm=llm, prompt=prompt_step3).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"context\": context3\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 4: Σχέδιο θεραπείας / διαχείρισης (με RAG και κατευθυντήριες οδηγίες)\n",
    "        context4 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output + step2_output + step3_output)[:15]])\n",
    "        step4_output = LLMChain(llm=llm, prompt=prompt_step4).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"context\": context4\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 5: Τελική απάντηση (MCQ) χωρίς χρήση RAG\n",
    "        step5_output = LLMChain(llm=llm, prompt=prompt_step5).invoke({\n",
    "            \"question\": question,\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"vindicate\": step3_output,\n",
    "            \"management\": step4_output\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 6: Αξιολόγηση της τελικής απάντησης\n",
    "        step6_evaluation = LLMChain(llm=llm, prompt=prompt_step6).invoke({\n",
    "            \"question\": question,\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"management\": step4_output,\n",
    "            \"answer\": step5_output\n",
    "        })[\"text\"]\n",
    "\n",
    "\n",
    "        # Εξοδος\n",
    "        output = {\n",
    "            \"question\": question,\n",
    "            \"target_output\": example.get(\"target_output\", \"\"),\n",
    "            \"step1_summary\": step1_output,\n",
    "            \"step2_diagnosis\": step2_output,\n",
    "            \"step3_vindicate_analysis\": step3_output,\n",
    "            \"step4_management\": step4_output,\n",
    "            \"step5_final_answer\": step5_output,\n",
    "            \"step6_evaluation\": step6_evaluation\n",
    "        }\n",
    "\n",
    "        fout.write(json.dumps(output) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
