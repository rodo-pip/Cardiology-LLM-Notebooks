{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8996-7be2-406e-8c00-69667ca7f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install boto3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce14f95-0882-4c32-8f38-acb46979d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0ecc-7b88-441d-a38c-7ce1f5178e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain[bedrock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a212-16f4-4692-9dfc-3255f1791c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28b5cd-fcc9-4972-8a3d-c2ab060bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from pydantic import Extra\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "\n",
    "# Σύνδεση με AWS Bedrock Runtime\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "\n",
    "class BedrockLLM(BaseLLM):\n",
    "    model_id: str = Field(...)\n",
    "    region: str = Field(...)\n",
    "\n",
    "    def __init__(self, model_arn: str, region: str = \"us-west-2\"):\n",
    "        # model_id = model_arn.split(\"/\")[-1]  # <-- ΤΟ ΣΩΣΤΟ!\n",
    "        # print(model_id)\n",
    "        super().__init__(model_id=model_arn, region=region)\n",
    "        object.__setattr__(self, \"client\", boto3.client(\"bedrock-runtime\", region_name=region))\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"bedrock-custom\"\n",
    "\n",
    "    # Μέθοδος για αποστολή prompt στο Bedrock και λήψη απάντησης\n",
    "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    body = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"max_gen_len\": 512,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.9\n",
    "                    }\n",
    "                    \n",
    "                    response = self.client.invoke_model(\n",
    "                        modelId=self.model_id\n",
    "                        ,  # <-- ΠΡΟΣΟΧΗ! ΟΧΙ ARN\n",
    "                        body=json.dumps(body).encode(\"utf-8\"),\n",
    "                        contentType=\"application/json\",\n",
    "                        accept=\"application/json\"\n",
    "                    )\n",
    "                    result = json.loads(response[\"body\"].read())\n",
    "                    text = result.get(\"generation\") or result.get(\"outputs\", [{}])[0].get(\"text\", str(result))\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "                except self.client.exceptions.ModelNotReadyException:\n",
    "                    wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                    print(f\"Model not ready. Retrying in {wait:.1f} seconds...\")\n",
    "                    time.sleep(wait)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Client error during invoke: {str(e)}\")\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "# Φλορτωση Dataset\n",
    "input_path = \"sagemaker_ft500vers2 (1) (1).jsonl\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Ορισμός διανυσματικού μοντέλου\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "\n",
    "# Φόρτωση ανακτητή\n",
    "retriever = FAISS.load_local(\n",
    "    \"e5largev2_rag_db\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever()\n",
    "\n",
    "# Επιλογή LLM\n",
    "llm = BedrockLLM(\n",
    "    #model_arn = \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/y9xa363z3o7r\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/1tfsf1vs44wd\",\n",
    "    region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Prompt templates\n",
    "prompt_step1 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "Given the following clinical case and question, write a concise summary focusing only on the relevant clinical findings:\n",
    "\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step2 = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"\"\"\n",
    "You are a medical expert. Based on the clinical summary below, provide the most likely diagnosis with 2–3 reasonable differential diagnoses. Explain your reasoning step by step.\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step3 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "Based on the following:\n",
    "- Summary: {summary}\n",
    "- Most likely diagnosis: {diagnosis}\n",
    "- Relevant guideline info: {context}\n",
    "\n",
    "Provide short-term and long-term management recommendations, including risk scores if applicable.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step4 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"management\"],\n",
    "    template=\"\"\"\n",
    "Based on the following case and options:\n",
    "\n",
    "CASE:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "MANAGEMENT PLAN:\n",
    "{management}\n",
    "\n",
    "Select the best answer choice and justify why it's correct and why the others are incorrect. Begin your answer with the letter (e.g., A.)\n",
    "\n",
    "FINAL ANSWER:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Αξιολόγηση\n",
    "output_path = \"validation_answers_e5large_multistep_reasoning_ft&nocontext.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for example in tqdm(dataset):\n",
    "        question = example[\"model_input\"]\n",
    "        # print(question)\n",
    "        step1_output = LLMChain(llm=llm, prompt=prompt_step1).run(question=question)\n",
    "        step2_output = LLMChain(llm=llm, prompt=prompt_step2).run(summary=step1_output)\n",
    "\n",
    "        docs = retriever.get_relevant_documents(step1_output + step2_output)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "\n",
    "        step3_output = LLMChain(llm=llm, prompt=prompt_step4).run(\n",
    "            summary=step1_output,\n",
    "            diagnosis=step2_output,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "        step4_output = LLMChain(llm=llm, prompt=prompt_step5).run(\n",
    "            question=question,\n",
    "            summary=step1_output,\n",
    "            management=step4_output\n",
    "        )\n",
    "        # print(step5_output)\n",
    "\n",
    "        output = {\n",
    "            \"question\": question,\n",
    "            \"target_output\": example.get(\"target_output\", \"\"),\n",
    "            \"step1_summary\": step1_output,\n",
    "            \"step2_diagnosis\": step2_output,\n",
    "            \"step3_management\": step4_output,\n",
    "            \"step4_final_answer\": step5_output\n",
    "        }\n",
    "        fout.write(json.dumps(output) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2ab55-4159-4e9c-b635-b99121a3180e",
   "metadata": {},
   "source": [
    "Ο σχεδιασμός του προτεινόμενου pipeline βασίζεται σε αρχές πολυπρακτορικών συστημάτων (multi-agent workflows), όπου κάθε υποσύστημα εκτελεί έναν σαφώς καθορισμένο, ανεξάρτητο ρόλο εντός της συνολικής ροής εργασίας. Η προσέγγιση αυτή ευθυγραμμίζεται με το μοντέλο «Modular Agent Design» που περιγράφει η Anthropic (2024), το οποίο προωθεί τον διαχωρισμό της λογικής σε διακριτά και αυτόνομα modules, έτσι ώστε κάθε agent να είναι όχι μόνο επαναχρησιμοποιήσιμος, αλλά και πλήρως αξιολογήσιμος ως προς την απόδοσή του. Ένα τέτοιο design διευκολύνει την ανίχνευση σφαλμάτων, την ενσωμάτωση μηχανισμών debugging και την εφαρμογή targeted βελτιώσεων χωρίς να επηρεάζεται το σύνολο του συστήματος. Επιπλέον, προωθεί τη δυνατότητα παραλληλισμού και επεκτασιμότητας, κάτι ιδιαίτερα σημαντικό σε περιβάλλοντα όπως η κλινική υποβοηθούμενη λήψη αποφάσεων.\n",
    "\n",
    "Ένα από τα βασικά καινοτόμα χαρακτηριστικά του συστήματος είναι η στρατηγική τοποθέτηση του retrieval βήματος όχι στην αρχική ερώτηση (prompt-level RAG), αλλά μετά από ένα πρώτο κύκλο reasoning που περιλαμβάνει σύνοψη και διάγνωση. Η προσέγγιση αυτή συνδέεται με μεθοδολογίες όπως το RAG-Fusion (Press et al., 2023), που ενισχύουν την ανάκτηση πληροφοριών μέσω query reformulation. Σύμφωνα με τις αρχές αυτές, η άμεση χρήση της αρχικής ερώτησης ενδέχεται να οδηγήσει σε retrieval documents που δεν απαντούν ουσιαστικά το ζητούμενο, ενώ η ενδιάμεση επεξεργασία (με τη μορφή structured reasoning) επιτρέπει τη δημιουργία πιο στοχευμένων queries για αναζήτηση. Παρόμοια συμπεράσματα καταγράφονται και στη μελέτη των Santhanam et al. (2023), όπου η χρήση intermediate thought steps πριν το retrieval οδηγεί σε αυξημένη σχετικότητα και απόδοση του τελικού συστήματος. Με άλλα λόγια, το σύστημα εφαρμόζει μια μορφή \"reasoning-first RAG\", η οποία επιτρέπει πιο context-aware ανακτήσεις και βελτιώνει την ακρίβεια των τελικών απαντήσεων.\n",
    "\n",
    "Η χρήση προκαθορισμένων prompts (prompt templates) ανά reasoning phase εντάσσεται στη στρατηγική του structured prompting, όπως παρουσιάζεται από τους Wei et al. (2022) και άλλους. Κάθε φάση (π.χ. clinical summary, diagnosis generation, guideline-based management, final decision) υποστηρίζεται από ένα tailor-made prompt που οδηγεί το LLM σε συγκεκριμένο τύπο reasoning. Αυτή η πρακτική όχι μόνο μειώνει το hallucination risk, αλλά διευκολύνει και τη traceability κάθε παραγόμενου reasoning βήματος. Επιπλέον, ενισχύεται η explainability της συνολικής απάντησης, καθώς η απόφαση τεκμηριώνεται σε βάση ενδιάμεσων reasoning stages. Τελική αξιολόγηση της απάντησης μπορεί να γίνει αυτόματα ή ημι-αυτόματα, χρησιμοποιώντας frameworks όπως το G-Eval (Liu et al., 2023) ή το RAGAS (Gao et al., 2023), τα οποία επιτρέπουν την αποτίμηση consistency, faithfulness και groundedness, ιδίως όταν υπάρχουν ενδιάμεσα structured outputs.\n",
    "\n",
    "Αναφορικά με την ανάκτηση εγγράφων, αξιοποιείται ένα εξειδικευμένο vector store με embeddings από το GatorTron (Yang et al., 2022), ένα LLM σχεδιασμένο για τον ιατρικό τομέα. Η επιλογή αυτή ενισχύει την απόδοση του retriever καθώς το vector space είναι προσαρμοσμένο σε κλινική γλώσσα και φρασεολογία. Με τον τρόπο αυτό, μειώνεται το semantic drift που παρατηρείται συχνά όταν χρησιμοποιούνται general-purpose embeddings (όπως BERT ή MiniLM) σε εξειδικευμένα domains. Η χρήση domain-specific models έχει τεκμηριωθεί βιβλιογραφικά ως κρίσιμος παράγοντας για επιτυχημένη γνώση ανάκτησης (Gao et al., 2023; Yang et al., 2022).\n",
    "\n",
    "Για την τελική παραγωγή της απάντησης χρησιμοποιείται το μοντέλο LLaMA3-70B-instruct μέσω της πλατφόρμας AWS Bedrock. Το μοντέλο αυτό, ως state-of-the-art open-weight LLM με fine-tuning σε instruction-following tasks, προσφέρει υψηλή απόδοση σε σύνθετα reasoning prompts, ιδίως όταν χρησιμοποιείται με step-by-step chains. Η απόφαση να υλοποιηθεί το pipeline με modular chains (LLMChain σε κάθε βήμα) αντί για μονολιθική prompting δομή επιτρέπει την ευελιξία στην τροποποίηση και επέκταση του workflow, όπως ενδεικνύουν οι Press et al. (2023) και οι οδηγίες της Anthropic (2024).\n",
    "\n",
    "Βιβλιογραφία:\n",
    "\n",
    "Anthropic. (2024). Building Effective Agents. https://www.anthropic.com/engineering/building-effective-agents\n",
    "\n",
    "Press, O., Du, Y., Liu, P. J., & Levy, O. (2023). RAG-Fusion: Towards Information-Rich Answers from Retrieval-Augmented Generation. arXiv:2305.12987\n",
    "\n",
    "Santhanam, K., Zeng, A., Zhang, Z., et al. (2023). Augmented Language Models: A Survey. arXiv:2302.07842\n",
    "\n",
    "Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903\n",
    "\n",
    "Liu, S., Wu, Y., Gao, J., & Liu, J. (2023). G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv:2303.16634\n",
    "\n",
    "Gao, J., Liu, S., Wang, X., et al. (2023). RAGAS: An Evaluation Framework for Retrieval-Augmented Generation. arXiv:2309.00393\n",
    "\n",
    "Yang, X., et al. (2022). GatorTron: A Large Language Model for Clinical Natural Language Processing. arXiv:2204.1235\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
