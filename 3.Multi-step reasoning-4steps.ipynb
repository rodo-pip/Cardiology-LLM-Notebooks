{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8996-7be2-406e-8c00-69667ca7f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install boto3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce14f95-0882-4c32-8f38-acb46979d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0ecc-7b88-441d-a38c-7ce1f5178e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain[bedrock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a212-16f4-4692-9dfc-3255f1791c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28b5cd-fcc9-4972-8a3d-c2ab060bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from pydantic import Extra\n",
    "from typing import Optional, List\n",
    "from pydantic import Field\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "\n",
    "# Σύνδεση με AWS Bedrock Runtime\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "\n",
    "class BedrockLLM(BaseLLM):\n",
    "    model_id: str = Field(...)\n",
    "    region: str = Field(...)\n",
    "\n",
    "    def __init__(self, model_arn: str, region: str = \"us-west-2\"):\n",
    "        # model_id = model_arn.split(\"/\")[-1]  # <-- ΤΟ ΣΩΣΤΟ!\n",
    "        # print(model_id)\n",
    "        super().__init__(model_id=model_arn, region=region)\n",
    "        object.__setattr__(self, \"client\", boto3.client(\"bedrock-runtime\", region_name=region))\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"bedrock-custom\"\n",
    "\n",
    "    # Μέθοδος για αποστολή prompt στο Bedrock και λήψη απάντησης\n",
    "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    body = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"max_gen_len\": 512,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.9\n",
    "                    }\n",
    "                    \n",
    "                    response = self.client.invoke_model(\n",
    "                        modelId=self.model_id\n",
    "                        ,  # <-- ΠΡΟΣΟΧΗ! ΟΧΙ ARN\n",
    "                        body=json.dumps(body).encode(\"utf-8\"),\n",
    "                        contentType=\"application/json\",\n",
    "                        accept=\"application/json\"\n",
    "                    )\n",
    "                    result = json.loads(response[\"body\"].read())\n",
    "                    text = result.get(\"generation\") or result.get(\"outputs\", [{}])[0].get(\"text\", str(result))\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "                except self.client.exceptions.ModelNotReadyException:\n",
    "                    wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                    print(f\"Model not ready. Retrying in {wait:.1f} seconds...\")\n",
    "                    time.sleep(wait)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Client error during invoke: {str(e)}\")\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "# Φλορτωση Dataset\n",
    "input_path = \"sagemaker_ft500vers2 (1) (1).jsonl\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Ορισμός διανυσματικού μοντέλου\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "\n",
    "# Φόρτωση ανακτητή\n",
    "retriever = FAISS.load_local(\n",
    "    \"e5largev2_rag_db\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever()\n",
    "\n",
    "# Επιλογή LLM\n",
    "llm = BedrockLLM(\n",
    "    #model_arn = \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/y9xa363z3o7r\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/1tfsf1vs44wd\",\n",
    "    region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# Prompt templates\n",
    "prompt_step1 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "Given the following clinical case and question, write a concise summary focusing only on the relevant clinical findings:\n",
    "\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step2 = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"\"\"\n",
    "You are a medical expert. Based on the clinical summary below, provide the most likely diagnosis with 2–3 reasonable differential diagnoses. Explain your reasoning step by step.\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step3 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "Based on the following:\n",
    "- Summary: {summary}\n",
    "- Most likely diagnosis: {diagnosis}\n",
    "- Relevant guideline info: {context}\n",
    "\n",
    "Provide short-term and long-term management recommendations, including risk scores if applicable.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step4 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"management\"],\n",
    "    template=\"\"\"\n",
    "Based on the following case and options:\n",
    "\n",
    "CASE:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "MANAGEMENT PLAN:\n",
    "{management}\n",
    "\n",
    "Select the best answer choice and justify why it's correct and why the others are incorrect. Begin your answer with the letter (e.g., A.)\n",
    "\n",
    "FINAL ANSWER:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Αξιολόγηση\n",
    "output_path = \"validation_answers_e5large_multistep_reasoning_ft&nocontext.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for example in tqdm(dataset):\n",
    "        question = example[\"model_input\"]\n",
    "        # print(question)\n",
    "        step1_output = LLMChain(llm=llm, prompt=prompt_step1).run(question=question)\n",
    "        step2_output = LLMChain(llm=llm, prompt=prompt_step2).run(summary=step1_output)\n",
    "\n",
    "        docs = retriever.get_relevant_documents(step1_output + step2_output)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "\n",
    "        step3_output = LLMChain(llm=llm, prompt=prompt_step4).run(\n",
    "            summary=step1_output,\n",
    "            diagnosis=step2_output,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "        step4_output = LLMChain(llm=llm, prompt=prompt_step5).run(\n",
    "            question=question,\n",
    "            summary=step1_output,\n",
    "            management=step4_output\n",
    "        )\n",
    "        # print(step5_output)\n",
    "\n",
    "        output = {\n",
    "            \"question\": question,\n",
    "            \"target_output\": example.get(\"target_output\", \"\"),\n",
    "            \"step1_summary\": step1_output,\n",
    "            \"step2_diagnosis\": step2_output,\n",
    "            \"step3_management\": step4_output,\n",
    "            \"step4_final_answer\": step5_output\n",
    "        }\n",
    "        fout.write(json.dumps(output) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
