{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8996-7be2-406e-8c00-69667ca7f4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install boto3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce14f95-0882-4c32-8f38-acb46979d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0ecc-7b88-441d-a38c-7ce1f5178e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain[bedrock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a212-16f4-4692-9dfc-3255f1791c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28b5cd-fcc9-4972-8a3d-c2ab060bc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import boto3\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.outputs import Generation, LLMResult\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class BedrockLLM(BaseLLM):\n",
    "    model_id: str = Field(...)\n",
    "    region: str = Field(...)\n",
    "\n",
    "    def __init__(self, model_arn: str, region: str = \"us-west-2\"):\n",
    "        super().__init__(model_id=model_arn, region=region)\n",
    "        object.__setattr__(self, \"client\", boto3.client(\"bedrock-runtime\", region_name=region))\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"bedrock-custom\"\n",
    "\n",
    "    def _generate(self, prompts: List[str], stop: Optional[List[str]] = None) -> LLMResult:\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    body = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"max_gen_len\": 512,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.9\n",
    "                    }\n",
    "                    response = self.client.invoke_model(\n",
    "                        modelId=self.model_id,\n",
    "                        body=json.dumps(body).encode(\"utf-8\"),\n",
    "                        contentType=\"application/json\",\n",
    "                        accept=\"application/json\"\n",
    "                    )\n",
    "                    result = json.loads(response[\"body\"].read())\n",
    "                    text = result.get(\"generation\") or result.get(\"outputs\", [{}])[0].get(\"text\", str(result))\n",
    "                    generations.append([Generation(text=text)])\n",
    "                    break\n",
    "                except self.client.exceptions.ModelNotReadyException:\n",
    "                    wait = 2 ** attempt + random.uniform(0, 1)\n",
    "                    print(f\"Model not ready. Retrying in {wait:.1f} seconds...\")\n",
    "                    time.sleep(wait)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Client error during invoke: {str(e)}\")\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "# Φόρτωση Δεδομένων\n",
    "with open(\"sagemaker_ft500vers2 (1) (1).jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# Ορισμός Διανυσματικού μοντέλου \n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\n",
    "\n",
    "# Φόρτωση Ανακτητή\n",
    "retriever = FAISS.load_local(\n",
    "    \"sapbert_base_db\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ").as_retriever()\n",
    "\n",
    "# Επιλογή LLM\n",
    "llm = BedrockLLM(\n",
    "    model_arn = \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/y9xa363z3o7r\",\n",
    "    #model_arn=\"arn:aws:bedrock:us-west-2:471112783210:imported-model/1tfsf1vs44wd\",\n",
    "    region=\"us-west-2\"\n",
    ")\n",
    "\n",
    "# === Prompt Templates ===\n",
    "prompt_step1 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "You are a clinical summarization expert. Carefully read the case and provide a concise summary of only the relevant clinical findings, omitting distractors.\n",
    "\n",
    "CASE AND QUESTION:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step2 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are a senior clinician. Using the summary and current guideline-based knowledge, identify:\n",
    "\n",
    "1. The most likely diagnosis (**bold**).\n",
    "2. 2–3 plausible differential diagnoses with one-line justifications.\n",
    "3. A logical, step-by-step reasoning for your top diagnosis.\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "DIAGNOSTIC REASONING:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step3 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are an expert clinician using the VINDICATE + P² mnemonic to evaluate other possible diagnoses.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "INITIAL DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "For each category below, state one plausible alternative diagnosis (or \"Not applicable\"):\n",
    "\n",
    "- Vascular, Infectious, Neoplastic, Degenerative, Intoxication/Iatrogenic, Congenital, Autoimmune/Allergic, Trauma, Endocrine/Metabolic, Psychiatric, Paraneoplastic.\n",
    "\n",
    "Conclude by stating if any alternative is more likely than the initial.\n",
    "\n",
    "VINDICATE + P² DIFFERENTIAL:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step4 = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"diagnosis\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are designing a clinical management plan.\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "RETRIEVED GUIDELINES:\n",
    "{context}\n",
    "\n",
    "Provide:\n",
    "\n",
    "1. Short-term treatment steps\n",
    "2. Long-term management goals\n",
    "3. Relevant clinical scores or red flags\n",
    "\n",
    "CLINICAL PLAN:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_step5 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"diagnosis\", \"vindicate\", \"management\"],\n",
    "    template=\"\"\"\n",
    "You are answering a multiple-choice medical exam question based strictly on the clinical information and reasoning below.\n",
    "\n",
    "CASE AND QUESTION:\n",
    "{question}\n",
    "\n",
    "CLINICAL SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSTIC REASONING:\n",
    "{diagnosis}\n",
    "\n",
    "DIFFERENTIAL ANALYSIS (VINDICATE):\n",
    "{vindicate}\n",
    "\n",
    "CLINICAL MANAGEMENT PLAN:\n",
    "{management}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- There are exactly 5 answer choices (A–E). Select only one.\n",
    "- Start your answer with the chosen letter (e.g., \"C.\") on the first line — no explanation yet.\n",
    "- Then, write 1–3 concise sentences justifying your choice.\n",
    "- Briefly reject the other 4 options.\n",
    "- Use ONLY the above information. Do NOT invent facts or make assumptions.\n",
    "\n",
    "FORMAT STRICTLY:\n",
    "<Letter>\n",
    "<Explanation>\n",
    "<Why others are incorrect>\n",
    "\n",
    "FINAL ANSWER:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_step6 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"summary\", \"diagnosis\", \"management\", \"answer\"],\n",
    "    template=\"\"\"\n",
    "You are a medical board evaluator. Review the following:\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "SUMMARY:\n",
    "{summary}\n",
    "\n",
    "DIAGNOSIS:\n",
    "{diagnosis}\n",
    "\n",
    "MANAGEMENT:\n",
    "{management}\n",
    "\n",
    "FINAL ANSWER:\n",
    "{answer}\n",
    "\n",
    "EVALUATE:\n",
    "- Is the answer choice consistent with the diagnosis and plan?\n",
    "- Is the reasoning complete and appropriate?\n",
    "- Are other options addressed clearly?\n",
    "\n",
    "Respond with only one of:\n",
    "- VALID: The final answer is correct and well justified.\n",
    "- INVALID: The answer is inconsistent or the justification is inadequate.\n",
    "- INCOMPLETE: The reasoning is partially correct but lacks completeness or clarity.\n",
    "\n",
    "EVALUATION:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# αΞΙΟΛΌΓΗΣΗ\n",
    "output_path = \"validation_answers_sapbert_6_steps.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for example in tqdm(dataset):\n",
    "        question = example[\"model_input\"]\n",
    "\n",
    "        # Βήμα 1: Περίληψη του κλινικού περιστατικού\n",
    "        step1_output = LLMChain(llm=llm, prompt=prompt_step1).invoke({\n",
    "            \"question\": question\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 2: Διάγνωση (με χρήση RAG και σχετικών αποσπασμάτων)\n",
    "        context2 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output)[:15]])\n",
    "        step2_output = LLMChain(llm=llm, prompt=prompt_step2).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"context\": context2\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 3: VINDICATE – συστηματικός έλεγχος πιθανών αιτιών (με RAG)\n",
    "        context3 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output + step2_output)[:15]])\n",
    "        step3_output = LLMChain(llm=llm, prompt=prompt_step3).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"context\": context3\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 4: Σχέδιο θεραπείας / διαχείρισης (με RAG και κατευθυντήριες οδηγίες)\n",
    "        context4 = \"\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(step1_output + step2_output + step3_output)[:15]])\n",
    "        step4_output = LLMChain(llm=llm, prompt=prompt_step4).invoke({\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"context\": context4\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 5: Τελική απάντηση (MCQ) χωρίς χρήση RAG\n",
    "        step5_output = LLMChain(llm=llm, prompt=prompt_step5).invoke({\n",
    "            \"question\": question,\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"vindicate\": step3_output,\n",
    "            \"management\": step4_output\n",
    "        })[\"text\"]\n",
    "\n",
    "        # Βήμα 6: Αξιολόγηση της τελικής απάντησης\n",
    "        step6_evaluation = LLMChain(llm=llm, prompt=prompt_step6).invoke({\n",
    "            \"question\": question,\n",
    "            \"summary\": step1_output,\n",
    "            \"diagnosis\": step2_output,\n",
    "            \"management\": step4_output,\n",
    "            \"answer\": step5_output\n",
    "        })[\"text\"]\n",
    "\n",
    "\n",
    "        # Εξοδος\n",
    "        output = {\n",
    "            \"question\": question,\n",
    "            \"target_output\": example.get(\"target_output\", \"\"),\n",
    "            \"step1_summary\": step1_output,\n",
    "            \"step2_diagnosis\": step2_output,\n",
    "            \"step3_vindicate_analysis\": step3_output,\n",
    "            \"step4_management\": step4_output,\n",
    "            \"step5_final_answer\": step5_output,\n",
    "            \"step6_evaluation\": step6_evaluation\n",
    "        }\n",
    "\n",
    "        fout.write(json.dumps(output) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2ab55-4159-4e9c-b635-b99121a3180e",
   "metadata": {},
   "source": [
    "Ο σχεδιασμός του προτεινόμενου pipeline βασίζεται σε αρχές πολυπρακτορικών συστημάτων (multi-agent workflows), όπου κάθε υποσύστημα εκτελεί έναν σαφώς καθορισμένο, ανεξάρτητο ρόλο εντός της συνολικής ροής εργασίας. Η προσέγγιση αυτή ευθυγραμμίζεται με το μοντέλο «Modular Agent Design» που περιγράφει η Anthropic (2024), το οποίο προωθεί τον διαχωρισμό της λογικής σε διακριτά και αυτόνομα modules, έτσι ώστε κάθε agent να είναι όχι μόνο επαναχρησιμοποιήσιμος, αλλά και πλήρως αξιολογήσιμος ως προς την απόδοσή του. Ένα τέτοιο design διευκολύνει την ανίχνευση σφαλμάτων, την ενσωμάτωση μηχανισμών debugging και την εφαρμογή targeted βελτιώσεων χωρίς να επηρεάζεται το σύνολο του συστήματος. Επιπλέον, προωθεί τη δυνατότητα παραλληλισμού και επεκτασιμότητας, κάτι ιδιαίτερα σημαντικό σε περιβάλλοντα όπως η κλινική υποβοηθούμενη λήψη αποφάσεων.\n",
    "\n",
    "Ένα από τα βασικά καινοτόμα χαρακτηριστικά του συστήματος είναι η στρατηγική τοποθέτηση του retrieval βήματος όχι στην αρχική ερώτηση (prompt-level RAG), αλλά μετά από ένα πρώτο κύκλο reasoning που περιλαμβάνει σύνοψη και διάγνωση. Η προσέγγιση αυτή συνδέεται με μεθοδολογίες όπως το RAG-Fusion (Press et al., 2023), που ενισχύουν την ανάκτηση πληροφοριών μέσω query reformulation. Σύμφωνα με τις αρχές αυτές, η άμεση χρήση της αρχικής ερώτησης ενδέχεται να οδηγήσει σε retrieval documents που δεν απαντούν ουσιαστικά το ζητούμενο, ενώ η ενδιάμεση επεξεργασία (με τη μορφή structured reasoning) επιτρέπει τη δημιουργία πιο στοχευμένων queries για αναζήτηση. Παρόμοια συμπεράσματα καταγράφονται και στη μελέτη των Santhanam et al. (2023), όπου η χρήση intermediate thought steps πριν το retrieval οδηγεί σε αυξημένη σχετικότητα και απόδοση του τελικού συστήματος. Με άλλα λόγια, το σύστημα εφαρμόζει μια μορφή \"reasoning-first RAG\", η οποία επιτρέπει πιο context-aware ανακτήσεις και βελτιώνει την ακρίβεια των τελικών απαντήσεων.\n",
    "\n",
    "Η χρήση προκαθορισμένων prompts (prompt templates) ανά reasoning phase εντάσσεται στη στρατηγική του structured prompting, όπως παρουσιάζεται από τους Wei et al. (2022) και άλλους. Κάθε φάση (π.χ. clinical summary, diagnosis generation, guideline-based management, final decision) υποστηρίζεται από ένα tailor-made prompt που οδηγεί το LLM σε συγκεκριμένο τύπο reasoning. Αυτή η πρακτική όχι μόνο μειώνει το hallucination risk, αλλά διευκολύνει και τη traceability κάθε παραγόμενου reasoning βήματος. Επιπλέον, ενισχύεται η explainability της συνολικής απάντησης, καθώς η απόφαση τεκμηριώνεται σε βάση ενδιάμεσων reasoning stages. Τελική αξιολόγηση της απάντησης μπορεί να γίνει αυτόματα ή ημι-αυτόματα, χρησιμοποιώντας frameworks όπως το G-Eval (Liu et al., 2023) ή το RAGAS (Gao et al., 2023), τα οποία επιτρέπουν την αποτίμηση consistency, faithfulness και groundedness, ιδίως όταν υπάρχουν ενδιάμεσα structured outputs.\n",
    "\n",
    "Αναφορικά με την ανάκτηση εγγράφων, αξιοποιείται ένα εξειδικευμένο vector store με embeddings από το GatorTron (Yang et al., 2022), ένα LLM σχεδιασμένο για τον ιατρικό τομέα. Η επιλογή αυτή ενισχύει την απόδοση του retriever καθώς το vector space είναι προσαρμοσμένο σε κλινική γλώσσα και φρασεολογία. Με τον τρόπο αυτό, μειώνεται το semantic drift που παρατηρείται συχνά όταν χρησιμοποιούνται general-purpose embeddings (όπως BERT ή MiniLM) σε εξειδικευμένα domains. Η χρήση domain-specific models έχει τεκμηριωθεί βιβλιογραφικά ως κρίσιμος παράγοντας για επιτυχημένη γνώση ανάκτησης (Gao et al., 2023; Yang et al., 2022).\n",
    "\n",
    "Για την τελική παραγωγή της απάντησης χρησιμοποιείται το μοντέλο LLaMA3-70B-instruct μέσω της πλατφόρμας AWS Bedrock. Το μοντέλο αυτό, ως state-of-the-art open-weight LLM με fine-tuning σε instruction-following tasks, προσφέρει υψηλή απόδοση σε σύνθετα reasoning prompts, ιδίως όταν χρησιμοποιείται με step-by-step chains. Η απόφαση να υλοποιηθεί το pipeline με modular chains (LLMChain σε κάθε βήμα) αντί για μονολιθική prompting δομή επιτρέπει την ευελιξία στην τροποποίηση και επέκταση του workflow, όπως ενδεικνύουν οι Press et al. (2023) και οι οδηγίες της Anthropic (2024).\n",
    "\n",
    "Βιβλιογραφία:\n",
    "\n",
    "Anthropic. (2024). Building Effective Agents. https://www.anthropic.com/engineering/building-effective-agents\n",
    "\n",
    "Press, O., Du, Y., Liu, P. J., & Levy, O. (2023). RAG-Fusion: Towards Information-Rich Answers from Retrieval-Augmented Generation. arXiv:2305.12987\n",
    "\n",
    "Santhanam, K., Zeng, A., Zhang, Z., et al. (2023). Augmented Language Models: A Survey. arXiv:2302.07842\n",
    "\n",
    "Wei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903\n",
    "\n",
    "Liu, S., Wu, Y., Gao, J., & Liu, J. (2023). G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv:2303.16634\n",
    "\n",
    "Gao, J., Liu, S., Wang, X., et al. (2023). RAGAS: An Evaluation Framework for Retrieval-Augmented Generation. arXiv:2309.00393\n",
    "\n",
    "Yang, X., et al. (2022). GatorTron: A Large Language Model for Clinical Natural Language Processing. arXiv:2204.1235\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
